---
title: Attention Is All You Need
category:
  - AI
tags:
  - [paper, DL]
math: true
date: 2023-09-25
---



# **Attention Is All You Need**

## 초록

​	대부분의 시퀀스 변환 모델은 인코더와 디코더를 포함한 복잡한 순환 또는 합성공 신경망에 기반한다. 가장 우수한 성능의 모델 역시 어텐션 메커니즘을 이용하여 인코더와 디코더를 연결한다. 우리는 순환 및 합성곱을 완전히 배제하고 오직 어텐션 메커니즘만을 사용한 간단한 신경망 모델 구조인 트랜스포머를 제안한다. 두 가지 기계 번역 작업에 대한 실험은 이 모델이 병렬화가 쉽고 훈련 시간을 상당히 감소시키기 때문에 우수한 성능을 냄을 보여준다. 우리의 모델은 `WMT 2014 English-to-German translation task`에서 앙상블을 포함한 현존 최고 점수를 2점 이상 넘어선 28.4점의 BLEU 점수를 달성했다. `WMT 2014 English-to-French translation task`에서 우리 모델은 8개의 GPU로 3.5일동안 훈련해 새로운 단일 최첨단 모델 BLEU 점수인 41.8점을 확립했다. 우리는 크고 제한된 데이터를 사용하여 영어 선거구를 구문 분석하는 것을 성공적으로 적용시키므로써 트랜스포머가 다른 문제도 잘 일반화할 수 있음을 보였다.



## Introduction

​	순환 신경망(RNN), 장단기 기억 신경망(LSTM) 그리고 게이트 순환 신경망(GRU)은 특히 언어 모델링이나 기계 번역같은 시퀀스 모델링과 변환 문제에서 최첨단 접근 방식으로 확고히 자리잡았다. 이후로도 순환 언어 모델과 인코더-디코더 구조의 한계를 넓히기 위해 수많은 노력이 계속되어왔다.

​	순환 모델은 일반적으로 입력과 출력 시퀀스의 기호 위치에 따라 연산을 고려한다. 연산 단계에 위치를 정렬하면서 이전 은닉 상태인 $h_{t-1}$과 위치 $t$의 입력을 기반으로 은닉 상태 $h_t$를 생성한다. 이 본질적으로 순차적인 특성은 훈련 예제 안에서의 병렬 처리를 배제하고, 이것은 시퀀스 길이가 길어질수록 치명적인데 메모리 제약으로 예제에서 일괄 처리의 한계가 생기기 때문이다. 최근 작업에서는 인수분해 트릭과 조건부 계산을 통해 계산 효율에서 상당한 개선을 달성했고, 후자의 경우 모델의 성능 또한 향상되었다. 그러나 순차 연산의 근본적인 문제는 아직도 남아있다.

​	어텐션 메커니즘은 입출력 시퀀스의 거리에 관계없이 종속성 모델링을 가능하게 함으로써 다양한 문제에서 강력한 시퀀스 모델링과 변환 모델의 필수적인 부분이 되었다. 그러나, 몇몇 경우를 제외한 모든 경우에서 이러한 어텐션 메커니즘은 순환 네트워크와 함께 사용된다.

​	이번 작업에서 우리는 입력과 출력 사이의 종속성을 도출하기 위해 순환을 배제하고 대신에 어텐션 메커니즘에 전적으로 의존하는 모델 구조인 트랜스포머를 제안한다. 트랜스포머는 8개의 P100 GPU로 12시간 정도 훈련시켜 상당히 좋은 병렬화와 번역 수준의 새로운 수준에 도달할 수 있었다.



## Background

​	순차 연산을 줄이는 목표는 기초 빌딩 블럭으로 합성곱 신경망을 사용하며 모든 입력과 출력 위치에 대해 병렬적으로 은닉 상태를 연산하는 Extended Neural GPU, ByteNet, ConvS2S를 기반으로 형성되었다. 이 모델들에서, 



## Model Architecture

​	대부분의 경쟁력 있는 신경망 시퀀스 변환 모델은 인코더-디코더 구조를 가지고 있다. 여기서 인코더는 기호 표현$(x_1,..,x_n)$ 시퀀스를 연속적인 표현 $z=(z_1,...,z_n)$ 시퀀스로 매핑한다. 그 다음 디코더는 주어진 $z (y_1,...,y_m)$ 시퀀스를 한 요소씩 기호로 생성한다. 각각의 과정에서 모델은 자기 회귀적이며, 다음 생성 시에 이전에 생성한 기호를 추가적인 입력으로 사용한다.

​	트랜스포머는 셀프 어텐션 층과 point-wise, 인코더와 디코더가 완전히 연결된 레이어를 사용한다. 이는 그림 1의 왼쪽과 오른쪽 절반에 나타나 있다.

### 3.1 Encoder and Decoder Stacks

- **인코더**: 인코더는 $N=6$인 동일한 레이어의 층으로 구성된다. 각각의 레이어는 두 개의 서브 레이어를 가지고 있다. 첫 번째는 멀티 헤드 셀프 어텐션 메커니즘이고, 두 번째는 간단한 position

### 3.2 Attention

​	어텐션 함수는 쿼리와 key-value 쌍의 집합을 출력으로 매핑하는 것으로 설명될 수 있고, 이때 쿼리, key, value, 출력값은 모두 벡터이다. 출력은 values의 가중 합으로 계산되며, 할당된 가중치는 상응하는 키의 compatibility function으로 계산된다.

#### 3.2.1 Scaled Dot-Product Attention

​	우리는 우리의 어텐션을 "Scaled Dot-Product Attention"으로 명명한다. 입력은 $d_k$ 차원의 쿼리와 키, $d_v$ 차원의 values로 구성된다. 우리는 쿼리를 모든 key들로 내적하고, 각각을 $d_k$의 제곱근으로 나눈다. 그리고 values에서 가중치를 얻기 위해 softmax 함수를 적용한다.

​	실제로, 우리는 쿼리의 집합을 행렬 Q에 묶어 어텐션 함수를 동시에 계산한다. 키와 값 역시 각각 행렬 K 와 V에 묶인다. 우리는 이 행렬을 출력으로 다음과 같이 계산한다:

(1)

 가장 일반적으로 사용되는 두 가지 어텐션 함수는 additive 어텐션과 dot-product(multiplicative) 어텐션이다. dot product 어텐션은 스케일링 요소를 제외하면 우리의 알고리즘과 동일하다. additive 어텐션은 단일 은닉층을 가진 feed-forward network를 사용하여 compatibility 함수를 연산한다. 이 두 가지의 이론적복잡도는 유사하지만, 실제로는 매우 최적화된 행렬 곱 코드를 사용하여 구현된 dot-product attention이 더 빠르고 공간 효율적이다.

​	작은 dk값에서는 두 메커니즘이 유사하게 수행되지만, 큰 dk값에서는 스케일링이 없으면 additive attention이 dot product attention을 능가한다.



#### 3.2.2 Multi-Head Attention

​	$d_{model}$ 차원의 키, 값, 쿼리의 단일 어텐션 함수를 사용하는 대신에, 우리는 서로 다른 학습된 선형 투영을 사용하여 $d_k$, $d_k$ 및 $d_v$ 차원으로 쿼리, 키 및 값을 h번 선형으로 투영하는 것이 유익하다는 것을 발견했다.



### 3.4 Embeddings and Softmax

​	다른 시퀀스 변환 모델과 유사하게, 우리는 입력 토큰과 출력 토큰을 $d_{model}$ 차원 벡터로 바꾸기 위해 **학습된 임베딩을 사용**했다. 우리는 또한 디코더 출력을 예측된 다음 토큰 확률로 바꾸기 위해 **일반적으로 학습된 선형 변환과 소프트맥스 함수를 사용**하였다. 우리의 모델에서, 우리는 두 개의 임베딩 레이어와 pre-softmax 선형 변환에 **같은 가중치 행렬을 공유**하였다. 임베딩 레이어에서, 우리는 **해당 가중치에 $\sqrt{d_model}$을 곱했다.**



### 3.5 Positional Encoding

​	우리 모델에는 **순환이나 합성곱이 없으므로**, 모델이 시퀀스의 순서를 활용하도록 하려면 우리는 반드시 시퀀스의 토큰에 **상대적이거나 절대적인 위치 정보를 주입**해야 한다. 이를 위해, 우리는 **"위치 인코딩"**을 입력 임베딩의 인코더와 디코더 층 맨 아래에 추가했다. 위치 인코딩은 임베딩과 동일한 차원 $d_{model}$을 가지므로 둘을 더할 수 있다. 위치 인코딩에는 많은 선택지가 있다: 학습된 것과 고정된 것.

​	이 작업에서, 우리는 다른 주파수의 **사인과 코사인 함수**를 사용할 수 있다.  


$$
PE_{(pos, 2i)}=\sin (pos/10000^{2i/d_{model}})
$$

$$
PE_{(pos,2i+1)}=\cos (pos/10000^{2i/d_{model}})
$$

$pos$는 위치이고 $i$는 차원이다. 즉, 위치 인코딩의 **각 차원은 정현파에 해당**한다. 파장은 $2\pi$에서 $10000\dot 2\pi$까지 기하학적인 진폭을 형성한다. 우리는 모델이 고정된 어떤 오프셋 k에서 $PE_{pos+k}$가 $PE\_{pos}$의 **선형함수로 표현**될 수 있기 때문에 상대적 위치를 기반으로 쉽게 학습될 수 있다고 가정했고, 따라서 이 함수를 선택했다.

​	우리는 또한 **학습된 위치 임베딩**을 대신 사용해보기도 했는데, 두 가지 버전이 거의 근접한 결과를 낸다는 것을 알 수 있었다. 우리는 모델이 훈련 중 만난 것보다 **더 긴 시퀀스도 추정**할 수 있도록 하는 정현파 함수를 선택했다.



## Why Self-Attention

​	이 섹션에서는 셀프 어텐션 레이어의 다양한 측면과 일반적으로 일반적인 시퀀스 변환 인코더나 디코더 속의 은닉층처럼 기호 표현$(x_1,...,x_n)$의 가변 길이 시퀀스를 같은 길이의 다른 시퀀스 $(z_1,...,z_n) (x_i , z_i \in R^d)$로 매핑하는 데 사용되는 순환 레이어나 합성곱 레이어를 비교해볼 것이다. 셀프 어텐션 사용을 위한 세 가지 요구 사항이 고려된다.

​	첫번째는 레이어 당 총 계산 복잡도이다. 두 번째는 순차적으로 수행하는 데 필요한 최소 수로 측정되는 병렬화 가능한 계산량이다.

​	세 번째는 네트워크 내 장거리 종속성 간의 경로 길이이다. 장거리 종속성을 학습하는 것은 많은 시퀀스 변환 문제의 핵심 과제이다. 그러한 종속성을 학습하는 능력에 영향을 미치는 핵심 요인 하나는 네트워크를 통과하는 역방향 및 순방향 신호의 경로 길이이다. 어떤 입력 및 출력 시퀀스의 위치 조합 사이의 거리가 짧을수록 장거리 종속성을 학습하기 쉬워진다. 따라서 우리는 다양한 다른 레이어 타입으로 구성된 네트워크의 두 입출력 사이의 경로 길이의 최댓값을 비교했다.

​	표 1에 언급한 것처럼, 셀프 어텐션 레이어는 모든 위치를 순차적으로 실행되는 작업의 상수로 연결하는 반면, 순환 레이어는 $O(n)$의 순차 작업을 요구한다. 계산 복잡도 측면에서, word-piece나 byte-pair 표현같은 기계 번역에서 최첨단 모델이 사용하는 문장 표현에서 자주 나타나는 시퀀스 길이 $n$이 표현 차원 $d$보다 작은 상황에서 셀프 어텐션 레이어는 순환 레이어보다 빠르다. 아주 긴 시퀀스와 관련된 문제에서의 계산 성능을 개선하기 위해, 셀프 어텐션은 각 출력 위치를 중심으로 한 입력 시퀀스에서의 크기 $r$ 근방의 이웃만 고려하도록 제한될 수 있다. 이것은 최대 경로 길이를 $O(n/r)$으로 증가시킬 수 있다. 우리는 향후 연구에서 이 접근을 더 조사할 계획이다.

​	커널 폭 $k$가 $n$보다 작은 단일 합성곱 레이어는 모든 입력과 출력 위치쌍과 연결되어있지 않다. 그렇게 하려면 연속된 커널의 경우 $O(n/k)$이나 확장된 합성곱의 경우 $O(\log_k(n))$층의 합성곱 레이어가 필요한데, 이는 네트워크 내의 두 위치 간 최장 경로 길이를 증가시킨다. 일반적으로 합성곱 레이어는 순환 레이어보다 $k$배 큰 비용을 필요로 한다. 그러나 분리 가능한 합성곱은 복잡도를 $O(k\dot n \dot d + n \dot d^2)$ 까지 상당히 줄인다.



## Training

​	이번 섹션에서는 우리 모델의 훈련 체제를 설명한다.

### 5.1 Training Data and Batching

​	우리는 약 450만개의 문장 쌍으로 구성된 표준 WMT 2014 English-German dataset에서 훈련했다. 문장은 바이트 페어 인코딩을 사용하여 인코딩되었으며, 공유된 소스-타겟 어휘에 약 37000개의 토큰이 사용되었다. English-French의 경우, 우리는 상당히 큰 WMT 2014 English-French 데이터셋을 사용했으며 3600만개의 문장과 32000 word-piece 어휘의 토큰으로 분할했다. 문장 쌍은 근사적인 시퀀스 길이로 함께 배치되었다. 각 훈련 배치는 약 25000개의 소스 토큰과 25000개의 타겟 토큰을 포함하는 문장 쌍의 집합을 포함했다.





