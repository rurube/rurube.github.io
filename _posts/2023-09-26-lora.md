---
title: LoRA: Low-Rank Adaptation of Large Language Models
category:
  - AI
tags:
  - [paper, DL]
math: true
date: 2023-09-26
---



# LoRA: Low-Rank Adaptation of Large Language Models

## Abstract

​	자연어 처리의 중요한 패러다임은 일반적인 도메인 데이터에서의 대규모 사전 훈련과 세부적인 과제 혹은 도메인에서의 적용으로 구성된다. 우리는 더 큰 모델을 사전 훈련하기 때문에 모든 파라미터를 다시 훈련하는 전체적인 미세 조정은 실현 가능성이 낮아진다. GPT-3 175B를 예로 들어 보자. 1750억개의 파라미터로 미세조정된 모델의 독립 인스턴스를 배포하는 것은 엄청나게 많은 비용이 든다. 우리는 **Lo**w-**R**ank **A**daption(LoRA)을 제안한다. LoRA는 

## 2. Problem statement

​	우리의 제안은 훈련 목표와 무관하지만 동기 부여 사례로서의 언어 모델링에 중점을 둔다. 다음은 언어 모델링 문제, 특히 작업별 프롬프트가 주어졌을 때 조건부 확률을 최대화하는 문제에 대한 간략한 설명이다.

​	

## 3. Aren't Existiong Solutions Good Enough?

